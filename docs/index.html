<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="VisPhyWorld: Probing physical reasoning in multimodal large language models via code-driven video reconstruction and simulation."
  />
  <meta name="keywords" content="VisPhyWorld, VisPhyBench, MLLM, physics reasoning, simulation, video reconstruction" />
  <title>VisPhyWorld</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
	  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
	  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
	  <link rel="stylesheet" href="./static/css/index.css" />
	  <link rel="icon" href="./static/images/robot.png" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <span>
                  Jiarong Liang<sup>*1</sup>&nbsp;&nbsp;
                  Max Ku<sup>*1</sup>&nbsp;&nbsp;
                  Ka-Hei Hui<sup>2</sup>&nbsp;&nbsp;
                  Ping Nie<sup>3</sup>&nbsp;&nbsp;
                  Wenhu Chen<sup>1</sup>
                </span>
              </span>
            </div>

	            <div class="is-size-6 publication-authors">
	              <span class="author-block"><sup>1</sup>University of Waterloo</span>&nbsp;&nbsp;
	              <span class="author-block"><sup>2</sup>Autodesk AI Lab</span>&nbsp;&nbsp;
	              <span class="author-block"><sup>3</sup>Independent</span>&nbsp;&nbsp;
	              <span class="eql-cntrb"><small><sup>*</sup>Equal contribution</small></span>
	            </div>

	            <div class="is-size-6 publication-authors">
	              <span class="author-block">
	                Corresponding to:
	                <a href="mailto:jiarongliangcs@gmail.com">jiarongliangcs@gmail.com</a>&nbsp;&nbsp;
					<a href="mailto:m3ku@uwaterloo.ca">m3ku@uwaterloo.ca</a>&nbsp;&nbsp;
	                <a href="mailto:wenhu.chen@uwaterloo.ca">wenhu.chen@uwaterloo.ca</a>
	              </span>
	            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="./static/pdfs/sample.pdf" target="_blank" rel="noopener" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
	                <span class="link-block">
	                  <a
	                    href="https://github.com/TIGER-AI-Lab/VisPhyWorld"
	                    target="_blank"
	                    rel="noopener"
	                    class="external-link button is-normal is-rounded is-dark"
	                  >
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
	                <span class="link-block">
	                  <a href="https://huggingface.co/collections/TIGER-Lab/visphyworld" target="_blank" rel="noopener" class="external-link button is-normal is-rounded is-dark">
	                    <span class="icon"><img class="hf-icon" src="./static/images/huggingface_logo.svg" alt="Hugging Face" /></span>
	                    <span>HuggingFace</span>
	                  </a>
	                </span>
                <span class="link-block">
                  <a href="#case-studies" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-images"></i></span>
                    <span>Example</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 section-title">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics
              remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question
              Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an
              explicit, testable physical hypothesis. We propose <strong>VisPhyWorld</strong>, an execution-based
              framework that evaluates physical reasoning by requiring models to generate executable simulator code from
              visual observations. By producing runnable code, the inferred world representation is directly
              inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on
              this framework, we introduce <strong>VisPhyBench</strong>, comprising 209 evaluation scenes derived from
              108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance
              and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on
              the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene
              understanding, they struggle to accurately infer physical parameters and to simulate consistent physical
              dynamics.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="visphyworld">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 section-title">VisPhyWorld</h2>
          <h3 class="title is-4">Overview</h3>
          <div class="content has-text-justified">
            <p>
              We propose VisPhyWorld, a framework that uses LLMs to interpret raw video frames and generate executable
              simulation code for predicting future motion. To our knowledge, this is the first paradigm that
              evaluates physical reasoning in MLLMs through code reconstruction and re-simulation. By making object
              states and dynamics explicit, VisPhyWorld provides a direct and interpretable view of a modelâ€™s physical
              understanding.
            </p>
			            <figure class="image" style="margin-top: 1rem;">
			              <img
			                style="display: block; margin: 0 auto; width: 92%; height: auto;"
			                src="./static/images/visphyworld2.png"
			                alt="VisPhyWorld framework overview"
			                loading="lazy"
			              />
			            </figure>
	            <h3 class="title is-4">Engine choice</h3>
	            <p>
	              Engine choice matters for physical fidelity: physics-native backends like Three.js and P5.js let the
	              generated program offload gravity, collisions, friction, and contact constraints to a rigid-body solver,
	              producing more physically consistent rollouts, whereas non-physics renderers (e.g., SVG/Manim) often
	              degrade into heuristic motion scripting with artifacts such as static objects or interpenetration.
	            </p>
		            <figure class="image" style="margin-top: 1rem;">
		              <img
		                style="display: block; margin: 0 auto; width: 70%; height: auto;"
		                src="./static/images/engine_eval_task10010.png"
		                alt="Engine evaluation example: physics-native backends vs non-physics renderers"
		                loading="lazy"
		              />
		            </figure>
	          </div>
	        </div>
	      </div>
	    </div>
	  </section>

  <section class="section" id="visphybench">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 section-title">VisPhyBench</h2>
          <h3 class="title is-4">Overview</h3>
          <div class="content has-text-justified">
	            <p>
	              To evaluate how well models reconstruct appearance and reproduce physically plausible motion, we
	              introduce VisPhyBench, a unified evaluation protocol comprising 209 scenes derived from 108 physical
	              templates that assesses physical understanding through the lens of code-driven resimulation in both 2D
	              and 3D scenes, integrating metrics from different aspects. Each scene is also annotated with a coarse
	              difficulty label (easy/medium/hard).
	            </p>

		            <figure class="image" style="margin-top: 1rem;">
		              <a href="./static/pdfs/teaser.pdf" target="_blank" rel="noopener">
		                <img
		                  style="display: block; margin: 0 auto; width: 92%; height: auto;"
		                  src="./static/images/teaser.png"
		                  alt="Teaser figure (click to open PDF)"
		                  loading="lazy"
		                />
		              </a>
		            </figure>

            <h3 class="title is-4">VisPhyBench Advantages</h3>
            <p>
              Existing work has evaluated physical reasoning using video prediction benchmarks that test whether
              predicted future dynamics remain consistent, and Violation-of-Expectation paradigms that measure whether
              physically impossible events trigger greater predictive surprise. These approaches fit generative world
              models with explicit prediction objectives, but they do not transfer cleanly to MLLMs, which primarily
              generate text rather than predictive distributions or future videos. More recent benchmarks extend this
              setting to generative video models, while MLLM-based evaluators typically frame assessment as recognition
              tasks such as visual question answering. Although effective for probing high-level physical knowledge,
              recognition-style protocols can conflate true physical reasoning with appearance-based heuristics and
              dataset bias. In contrast, our framework improves interpretability by requiring models to output
              explicit, executable physical hypotheses that are validated through simulation, making success and
              failure easier to attribute.
            </p>

		            <figure class="image" style="margin-top: 1rem;">
		              <img
		                style="display: block; margin: 0 auto; width: 85%; height: auto;"
		                src="./static/images/table1.png"
		                alt="Table 1"
		                loading="lazy"
		              />
		            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="experiment-results">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 section-title">Experiment Results</h2>
          <h3 class="title is-4">Main Results</h3>
          <div class="content has-text-justified">
            <p>
              Table 2 summarizes performance across five metric families on VisPhyBench. Overall, most models achieve
              strong reconstruction and perceptual scores and maintain reasonable visual-semantic consistency; these
              results support our central claim that, once the task is cast as executable hypotheses under a fixed
              physics engine, most modern MLLM can reconstruct synthetic physical events with high fidelity, and the
              remaining gaps become diagnosable rather than opaque.
            </p>

		            <figure class="image" style="margin-top: 1rem;">
		              <img
		                style="display: block; margin: 0 auto; width: 100%; height: auto;"
		                src="./static/images/table2.png"
		                alt="Table 2"
		                loading="lazy"
		              />
		            </figure>

            <h3 class="title is-4">Motion and physical plausibility.</h3>
            <p>
              Assessing physical plausibility requires balancing motion statistics with perceptual coherence, since any single metric can be misleading. RAFT-EPE captures optical-flow agreement, yet some models achieve deceptively good flow by producing static or low-information outputs, while others match appearance well but violate event logic. We therefore adopt a joint evaluation strategy: models demonstrate valid physical understanding only when they exhibit both correct motion dynamics (low flow error) and holistic physical coherence under the Gemini rubric. Our case studies support this criterion: GPT-5 in Three.js faithfully simulates collision-driven interactions, whereas pixel-space baselines can look semantically plausible yet hallucinate dynamics; conversely, Qwen3-VL-Plus can appear favorable under flow alone but is penalized by holistic judgment. The same dissociation persists in perspective-rendered 3D scenes with depth-dependent contacts and occlusions, where semantic similarity fails to separate physically incorrect reconstructions. Overall, credible physical understanding is evidenced only when motion fidelity and holistic visual plausibility are simultaneously satisfied, and 3D scenes are necessary to stress-test reconstruction-based physical reasoning.
            </p>

	            <div class="columns is-variable is-3">
	              <div class="column">
	                <figure class="image">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto;"
		                    src="./static/images/case-study-task10063-000.png"
		                    alt="Case study: task10063_000"
		                    loading="lazy"
		                  />
	                </figure>
	              </div>
	              <div class="column">
	                <figure class="image">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto;"
		                    src="./static/images/case-study-dataset3d-14-tmpl-14.png"
		                    alt="Case study: dataset3D 14_tmpl_14"
		                    loading="lazy"
		                  />
                </figure>
              </div>
            </div>
          </div>

          <h3 class="title is-4" id="case-studies">Case Study Examples</h3>
          <div class="content">
	            <div id="case-study-carousel" class="carousel results-carousel">
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_1.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_1.png"
		                    alt="Case study 1"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_2.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_2.png"
		                    alt="Case study 2"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_3.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_3.png"
		                    alt="Case study 3"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_4.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_4.png"
		                    alt="Case study 4"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_5.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_5.png"
		                    alt="Case study 5"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_6.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_6.png"
		                    alt="Case study 6"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_7.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_7.png"
		                    alt="Case study 7"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	              <div class="item">
	                <a href="./static/pdfs/case_studies/case_study_8.pdf" target="_blank" rel="noopener">
		                  <img
		                    style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 620px; object-fit: contain;"
		                    src="./static/images/case_studies/case_study_8.png"
		                    alt="Case study 8"
		                    loading="lazy"
		                  />
	                </a>
	              </div>
	            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{visphyworld2026,
  title   = {VisPhyWorld},
  author  = {VisPhyWorld Team},
  year    = {2026},
  note    = {Project page},
  url     = {https://github.com/TIGER-AI-Lab/VisPhyWorld}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/TIGER-AI-Lab/VisPhyWorld" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page is built from the
              <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noopener">Nerfies project page template</a>.
              Please link back to the original template in the footer if you reuse it.
            </p>
            <p>
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
